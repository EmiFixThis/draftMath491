\subsection{Short Integer Solution (SIS)}

The Short Integer Solution Problem (SIS), was the first case where a connection between the average and worst-case instances of a problem was discovered. The connection was discovered by Mikolos Ajtai in 1996 \cite{Ajt1996} and serves as the basis for a variety of problems throughout the various applications of cryptography. Examples include collision-resistant hash functions, authentication and identification schemes, digital signatures, and countless others with the exception of any application to a public-key scheme.

The problem posed by SIS is as follows:

\begin{que}{Short Integer Solution}
    Given a finite number of elements drawn at uniformly and randomly from a certain finite additive subgroup, find a sufficiently `short' nonzero (nontrivial) linear combination of these elements which can be summed to zero. 
\end{que}


Formally, SIS is a parametrization of integers $n$ and $q$ which define the subgroup $\mathbb{Z}_{q}^{n}$ for a real number $\beta$ and a some number $m$ of group elements. Where $n \leq 100$ is the hardness parameter, and $q > \beta$ is some small polynomial dependent on $n$. 


\begin{defn}{Short Integer Solution}
    Given $m$ number of uniform, random vectors $a_{i} \in \mathbb{Z}_{q}^{n}$, which are column vectors of a matrix $M \in \mathbb{Z}_{q}^{n\times m}$, find a nonzero vector $z \in \mathbb{Z}^{m}$ with norm  $\rVert \leq \beta$ such that
    
    \[ f_{M}(z) \mathrel{\mathop:}= Mz = \sum_{i} a_{i}z_{i} = 0 \in \mathbb{Z}_{q}^{n} \]
\end{defn}



\begin{nt}
    Observe that the constraint $\lVert z \rVert$, is necessary or the problem can be easily solved by Gaussian elimination. Likewise if $q \nleq \beta$ then we have a trivial solution $z = \{q,0, \ldots,0\} \in \mathbb{Z}^{m}$, since any such solution for $M$ can be converted into any other solution $\[M | M' \]$ by padding the solution with zeros, which has the useless result that the norm is never altered in the least and so as $m$ becomes large the problem gets easier; in the reverse direction if $n$ becomes large the problem becomes harder. \footnote{If this reminds you at all of the caterpillar from Alice in Wonderland here, that would be entirely appropriate.} The bound on the norm, and the $m$ number of vectors must be large enough to ensure a solution exists. 
\end{nt}


\subsection{Learning with Parity (LWP)}


An algorithm trained to solve the Learning with Parity problem is given the following parameters:

\begin{itemize}
	\item Samples $(x, f(x))$, where the samples are generated by some distribution over the input.
	\item For the function $f$ the algorithm has the \emph{assurance} that $f$ can compute the parity of bits at one or more fixed locations.
\end{itemize}

If the algorithm solves the LWP problem it has successfully guessed the original function $f$ from the given samples.

The above version of LWP is easy to solve using Gaussian elimination provided the algorithm is given enough samples, and the distribution from which the samples are drawn is not overly skewed. 

\subsection{Noisy Learning with Parity}

Noisy Learning with Parity (NLWP) deviates slightly from the constraints given above significantly altering the solvability of the problem. 

The algorithm's parameters are adjusted as follows:


\begin{cond}
	The algorithm is provided with samples $(x,y)$, such that $y = 1- f(x)$ has only a small probability of knowing the parity of bits at some locations. 
\end{cond}

\begin{cond}
    These samples are permitted to be noisy, that is they may contain errors.
\end{cond}

The adjustments to the LWP problem change the expected outcome from one which is relatively easy to solve to one which is  conjectured to be hard to solve. 


\subsection{Learning with Errors (LWE)}

Learning with Errors is a lattice problem based upon the above machine learning problem of Noisy Learning with Parity (NLWP). 

LWE was described and shown to be hard in the worst-case by Oded Regev in 2009 \cite{Reg2009}. 
LWE, has a quantum reduction from the Shortest Vector Problem (SVP) which is known to be NP-hard. LWE forms the basis of security for many other algorithms which gain SVP as basis for their own security.


The definition of the Learning With Errors problem asks for a way to distinguish between random linear equations, which have been perturbed by the addition of some small amount of noise. Regev proved LWE is \emph{as hard to solve} as certain worst-case lattice problems.


\begin{lemm}{Regev's Main Lemma}
    There is an algorithm which takes a basis $B$ of an n-dimensional lattice $\Lambda = \Lambda(B)$, the parameter $r >> \frac{q}{lambda_{1}(\Lambda)}$, and a point vector $x \in \mathbb{R}^{n}$ where the distribution $dist(x, \lambda) < \frac{\alpha q}{\sqrt{2r}}$, are taken as input and where $\alpha$ is the noise parameter, $n$ is the security parameter, and $q$ is the modulus.
    
    The $dist(x, \lambda) < \frac{\alpha q}{\sqrt{2r}}$ has access to two oracles: 
    one oracle is not related to the input lattice and solves $LWE[n, \alpha, q]$ 
    
    The other, uses a continuous distribution $D_{r}$, where $r$ is the standard deviation. And $D_{\Lambda,r}$ is a discrete distribution over the lattice, such that all vectors $z \in \Lambda$ has a probability mass proprtionate to $D_{r}(z)$. Then the second oracle is a sampler which is specific to a given lattice.
    
For the inputs, and oracles that are given above. LWE efficiently finds the unique point $v \in \Lambda$ which is the closest point to $x$. Further, LWE finds this point with a large probability. 
\end{lemm}


However, despite that LWE has allowed for many new, and secure constructions it suffers from a large quadratic overhead cost (it is inefficient). For this reason the hypothesis by Chris Peikert was made that perhaps if the lattices were permitted to have some additional structure the new applications which based their security on the worst-case quantum hardness of LWE could be made to be efficient, perhaps even competative with current classical schemes.


\subsection{Ring-LWE (RLWE)}

Peikert's conjecture concerning the addition of structure to the general lattices of LWE, indeed produced a set of lattices defined over rings which allowed the new applications a much greater range of efficiency. Provided that one made the assumption that ring-based lattices retained the worst-case quantum hardness afforded to LWE. 




\subsection{IR-LWE}

In cases where the polynomial $\Phi(x)$ is a cyclotomic polynomial, the difficulty of solving the search version of RLWE problem is equivalent to finding some short vector vector in an ideal lattice formed from elements of $ Z[x]/\Phi (x) $ represented as integer vectors.

This problem is commonly known as the \emph{Approximate Shortest Vector Problem} ($\alpha$-SVP), where the problem is to find some vector in the lattice with length that is smaller than a multiple of $\alpha$ and the \emph{shortest vector} in the lattice. 

