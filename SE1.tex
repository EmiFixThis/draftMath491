\section{Learning with Parity (LWP)}

\todo[inline]{Introduce this section? How did we get from math problems broken by Shor's to LWP?}

An algorithm trained to solve the Learning with Parity\todo[inline]{What is this problem?}problem is given the following parameters:
\begin{itemize}
	\item Samples $(x, f(x))$, where the samples are generated by some distribution over the input.
	\item For the function $f$ the algorithm has the \emph{assurance} that $f$ can compute the parity of bits at one or more fixed locations.
\end{itemize}

If the algorithm solves the LWP problem it has sucessfully guessed the original function $f$ from the given samples.

The above version of LWP is easy to solve using Gaussian elimination provided the algorithm is given enough samples, and the distribution from which the samples are drawn is not overly skewed. 

\subsection{Noisy Learning with Parity}

Noisy Learning with Parity (NLWP) deviates slightly from the constraints given above significantly alterning the solvability of the problem. 

The algorithm's parameters are adjusted as follows:
\begin{itemize}
	\item The algorithm is provided with samples $(x,y)$, such that $y = 1- f(x)$ has only a small probability of knowing the parity of bits at some locations. 
	\item Additionally, these samples may contain errors (noise).
\end{itemize}

The adjustments to the LWP problem change the expected outcome from easy to solve to that of one conjectured to be hard to solve. 

\section{Learning with Errors (LWE)}

Learning with Errors is a lattice problem based upon the machine learning problem called Noisy Learning with Parity (NLWP). 
LWE, has a quantum reduction from SVP. LWE forms the basis of security for many other algorithms which gain SVP as basis for their own security.\todo[inline]{This section doesn't really describe LWE? I still don't really know what it is.}

\subsection{Ring-LWE (RLWE)}



\subsubsection{Peikerts Scheme}
